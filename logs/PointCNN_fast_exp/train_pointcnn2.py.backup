#!/usr/bin/env python
# -*- coding: utf-8 -*-
import shutil
import argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
import sklearn.metrics as metrics
from tqdm import tqdm
import os.path as osp
from data.data_utils import ModelNet40
import data.data_utils as dutil

# from models.pointcnn import RandPointCNN_cls, get_loss

from pointfield.model import CombinedModel
from pointfield.train_utils import log_init
""""""
import torch.nn.functional as F
from models.pointcnn import config
from models.pointcnn import modelnet_x3_l4


BASE_DIR = '/'.join(osp.abspath(__file__).split('\\')[0:-1])

def parse_args():
    '''PARAMETERS'''
    parser = argparse.ArgumentParser(description='Relation-Shape CNN Pointcloud Classification Training')
    parser.add_argument('--exp_name',   type=str,   default='PointCNN_fast_exp', help='Name of the experiment')
    parser.add_argument('--num_workers',type=int,   default=8,              help='num of workers [default: 8]')
    parser.add_argument('--num_points', type=int,   default=1024,           help='num of points to use [default: 1024]')
    parser.add_argument('--batch_size', type=int,   default=32,             help='Size of batch [default: 32]')
    parser.add_argument('--epochs',     type=int,   default=200,            help='number of episode to train ')
    parser.add_argument('--normal',     type=bool,  default=False,          help='Whether to use normal information [default: False]')
    parser.add_argument('--seed',       type=int,   default=1,              help='random seed (default: 1)')
    parser.add_argument('--cuda',       type=bool,  default=True,           help='enables CUDA training')
    parser.add_argument('--lr',         type=float, default=0.001,          help='Initial learning rate [default: 0.001]')
    parser.add_argument('--momentum',   type=float, default=0.9,            help='Initial learning rate [default: 0.9]')
    parser.add_argument('--optimizer',  type=str,   default='adam',         help='adam or momentum [default: adam]')
    parser.add_argument('--decay_rate', type=float, default=0.7,            help='Decay rate for lr decay [default: 0.8]')

    return parser.parse_args()


def main():
    #--- Training settings
    args = parse_args()
    logger = log_init(exp_name=args.exp_name, trainfile=__file__)
    logger.cprint('PARAMETERS...')
    logger.cprint(args)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed(args.seed)
    device = torch.device("cuda" if args.cuda else "cpu")

    # --- DataLoader
    print('Load dataset...')
    trans = transforms.Compose([  dutil.jitter_pointcloud])
    train_loader = DataLoader(  ModelNet40(partition='train', num_points=args.num_points, transform=trans),
                                num_workers=args.num_workers,   batch_size=args.batch_size,
                                shuffle=True,   drop_last=True)
    test_loader = DataLoader(   ModelNet40(partition='test', num_points=args.num_points),
                                num_workers=args.num_workers,   batch_size=args.batch_size,
                                shuffle=False,   drop_last=False)

    # --- Create Model
    classifier = modelnet_x3_l4().to(device)
    criterion = F.cross_entropy
    comb_model = CombinedModel(classifier).to(device)

    # --- Optimizer
    # if args.optimizer == 'Adam':
    #     optimizer = torch.optim.Adam(
    #         comb_model.parameters(),
    #         lr=args.lr,
    #         betas=(0.9, 0.999),
    #         eps=1e-08,
    #         weight_decay=args.decay_rate
    #     )
    # else:
    #     optimizer = torch.optim.SGD(comb_model.parameters(), lr=0.01, momentum=0.9)


    if config.train.optimizer == 'SGD':
        optimizer = torch.optim.SGD(comb_model.parameters(), config.train.learning_rate_base, momentum=config.train.momentum)
    elif config.train.optimizer == 'ADAM':
        optimizer = torch.optim.Adam(comb_model.parameters(), lr=config.train.learning_rate_base,eps=config.train.epsilon,
                                    weight_decay=config.train.weight_decay)
    else:
        raise NotImplementedError

    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)

    # --- Load Checkpoint
    try:
        checkpoint = torch.load(BASE_DIR+'/logs/%s/checkpoints/model.t7' % args.exp_name)
        start_epoch = checkpoint['epoch']
        best_inst_acc = checkpoint['inst_acc']
        best_class_acc = checkpoint['class_acc']
        comb_model.classifier.load_state_dict(checkpoint['classifier_state_dict'])
        comb_model.pointfield.load_state_dict(checkpoint['pointfield_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        logger.cprint('Use pretrain model')
    except:
        logger.cprint('No existing model, starting training from scratch...')
        start_epoch = 0
        best_inst_acc = 0
        best_class_acc = 0

    # --- train
    logger.cprint('Start training...')
    for epoch in range(start_epoch, args.epochs):
        logger.cprint('Epoch %d/%s:' % (epoch + 1, args.epochs))

        # train epoch
        train_pred = []
        train_true = []
        with tqdm(train_loader, total=len(train_loader), smoothing=0.9) as t:
            comb_model.train()

            for data, label in t:
                data, label = data.to(device), label.to(device).squeeze()

                optimizer.zero_grad()
                preds = comb_model(data, label).mean(dim=1)  ## preds - B x c
                cls_loss = criterion(preds, label)
                loss = cls_loss + comb_model.tri_loss + comb_model.reg_loss
                loss.backward()
                optimizer.step()

                preds = preds.max(dim=1)[1]  ## preds - B x 1
                train_true.append(label.cpu().numpy())
                train_pred.append(preds.detach().cpu().numpy())
                t.set_postfix(  cls_loss = cls_loss.item(),
                                tri_loss = comb_model.tri_loss.item(),
                                reg_loss = comb_model.reg_loss.item())
        scheduler.step()
        train_true = np.concatenate(train_true)
        train_pred = np.concatenate(train_pred)
        train_inst_acc = metrics.accuracy_score( train_true, train_pred)
        logger.cprint('Train Instance Accuracy: %f' % train_inst_acc)

        # test epoch
        test_pred = []
        test_true = []
        with torch.no_grad():
            comb_model.eval()
            for data, label in tqdm(test_loader, total=len(test_loader)):
                data, label = data.to(device), label.to(device).squeeze()
                preds = comb_model(data).mean(dim=1)
                preds = preds.max(dim=1)[1]
                test_true.append(label.cpu().numpy())
                test_pred.append(preds.detach().cpu().numpy())

        test_true = np.concatenate(test_true)
        test_pred = np.concatenate(test_pred)
        test_inst_acc = metrics.accuracy_score(test_true, test_pred)
        test_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)

        # Save Checkpoint
        save_dir = BASE_DIR+'/logs/%s/checkpoints/' % args.exp_name
        if (test_class_acc > best_class_acc):
            best_class_acc = test_class_acc
        if test_inst_acc>=best_inst_acc or (epoch+1)%10==0:
            logger.cprint('Saving model at %smodel.t7' % save_dir)
            if test_inst_acc >= best_inst_acc:
                best_inst_acc = test_inst_acc
            state = {
                'epoch': epoch + 1,
                'inst_acc': test_inst_acc,
                'class_acc': test_class_acc,
                'classifier_state_dict': comb_model.classifier.state_dict(),
                'pointfield_state_dict': comb_model.pointfield.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict()
            }
            torch.save(state, save_dir+'model.t7')
            if test_inst_acc >= best_inst_acc:
                logger.cprint('Saving best model at %sbest_model.t7' % save_dir)
                shutil.copy(save_dir+'model.t7', save_dir+'best_model.t7')

        logger.cprint('Test Instance Accuracy: %f, Class Accuracy: %f'% (test_inst_acc, test_class_acc))
        logger.cprint('Best Instance Accuracy: %f, Class Accuracy: %f'% (best_inst_acc, best_class_acc))
    logger.cprint('End of training...')


if __name__ == "__main__":
    main()